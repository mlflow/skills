"""
Generate a template script for running agent evaluation.

This script creates a customized Python script that executes the agent
on an evaluation dataset and collects trace IDs for scoring.

Usage:
    python run_evaluation_template.py                                        # Auto-detect everything
    python run_evaluation_template.py --module my_agent.agent                # Specify module
    python run_evaluation_template.py --entry-point run_agent                # Specify entry point
    python run_evaluation_template.py --dataset-name my-dataset              # Specify dataset
    python run_evaluation_template.py --module my_agent --entry-point run_agent --dataset-name my-dataset
"""

import argparse
import os
import subprocess
import sys

from utils import validate_env_vars


def list_datasets() -> list[str]:
    """List available datasets in the experiment."""
    try:
        code = """
import os
from mlflow import MlflowClient

client = MlflowClient()
experiment_id = os.getenv("MLFLOW_EXPERIMENT_ID")

datasets = client.search_datasets(experiment_ids=[experiment_id])
for dataset in datasets:
    print(dataset.name)
"""
        result = subprocess.run(["python", "-c", code], capture_output=True, text=True, check=True)
        return [line.strip() for line in result.stdout.strip().split("\n") if line.strip()]
    except Exception:
        return []


def generate_evaluation_code(
    tracking_uri: str, experiment_id: str, dataset_name: str, agent_module: str, entry_point: str
) -> str:
    """Generate Python code for running evaluation using mlflow.genai.evaluate()."""

    return f'''#!/usr/bin/env python3
"""
Run agent evaluation using mlflow.genai.evaluate().

Generated by run_evaluation_template.py

IMPORTANT: The entry point function signature must match the dataset's input keys.
For example, if your dataset has {{"inputs": {{"query": "..."}}}}, your entry point
must accept `query` as a keyword argument: def run_agent(query): ...
"""

import os
import sys
import mlflow
from mlflow.genai.datasets import get_dataset
from mlflow.genai.scorers import list_scorers

# Set environment variables
os.environ["MLFLOW_TRACKING_URI"] = "{tracking_uri}"
os.environ["MLFLOW_EXPERIMENT_ID"] = "{experiment_id}"

print("=" * 60)
print("MLflow Agent Evaluation")
print("=" * 60)
print()

# Configuration
DATASET_NAME = "{dataset_name}"
EXPERIMENT_ID = "{experiment_id}"

# Load dataset
print("Loading evaluation dataset...")
try:
    dataset = get_dataset(DATASET_NAME)
    df = dataset.to_df()
    print(f"  Dataset: {{DATASET_NAME}}")
    print(f"  Records: {{len(df)}}")

    # Show sample input structure
    if len(df) > 0:
        sample_inputs = df.iloc[0]['inputs']
        print(f"  Input keys: {{list(sample_inputs.keys())}}")
    print()
except Exception as e:
    print(f"✗ Failed to load dataset: {{e}}")
    sys.exit(1)

# Get registered scorers from the experiment
print("Loading registered scorers...")
try:
    registered_scorers = list_scorers(experiment_id=EXPERIMENT_ID)

    if not registered_scorers:
        print("  ⚠ No registered scorers found in experiment")
        print("  Register scorers first with: scorer.register()")
        sys.exit(1)

    print(f"  Found {{len(registered_scorers)}} registered scorer(s):")
    for scorer in registered_scorers:
        print(f"    - {{scorer.name}}")
    print()
except Exception as e:
    print(f"✗ Failed to load scorers: {{e}}")
    sys.exit(1)

# Import the agent entry point
# IMPORTANT: The function signature must match the dataset's input keys
print("Importing agent entry point...")
try:
    from {agent_module} import {entry_point}
    print(f"  ✓ Imported {entry_point} from {agent_module}")
    print()
except ImportError as e:
    print(f"✗ Failed to import agent: {{e}}")
    sys.exit(1)

# Run evaluation
print("=" * 60)
print("Running Evaluation")
print("=" * 60)
print()
print(f"  Dataset: {{DATASET_NAME}} ({{len(df)}} records)")
print(f"  Scorers: {{[s.name for s in registered_scorers]}}")
print(f"  Entry point: {agent_module}.{entry_point}")
print()

try:
    results = mlflow.genai.evaluate(
        data=df,
        predict_fn={entry_point},
        scorers=registered_scorers,
    )

    print()
    print("=" * 60)
    print("Evaluation Results")
    print("=" * 60)
    print()

    # Display aggregate metrics
    if hasattr(results, 'metrics') and results.metrics:
        print("Aggregate Metrics:")
        for metric_name, value in results.metrics.items():
            if isinstance(value, float):
                print(f"  {{metric_name}}: {{value:.3f}}")
            else:
                print(f"  {{metric_name}}: {{value}}")
        print()

    # Save detailed results
    if hasattr(results, 'eval_results_table'):
        output_file = "evaluation_results.csv"
        results.eval_results_table.to_csv(output_file, index=False)
        print(f"✓ Detailed results saved to: {{output_file}}")

    print()
    print("=" * 60)
    print("Evaluation Complete")
    print("=" * 60)

except Exception as e:
    print(f"✗ Evaluation failed: {{e}}")
    print()
    print("Common issues:")
    print("  1. Function signature mismatch - entry point params must match dataset input keys")
    print(f"     Dataset input keys: {{list(sample_inputs.keys()) if 'sample_inputs' in dir() else 'unknown'}}")
    print("     Entry point must accept these as keyword arguments")
    print("  2. Missing dependencies - check agent imports")
    print("  3. Authentication issues - verify OPENAI_API_KEY or other credentials")
    import traceback
    traceback.print_exc()
    sys.exit(1)
'''


def main():
    """Main workflow."""
    # Parse command-line arguments
    parser = argparse.ArgumentParser(
        description="Generate evaluation execution template script",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument("--module", help="Agent module name (e.g., 'my_agent.agent')")
    parser.add_argument("--entry-point", help="Entry point function name (e.g., 'run_agent')")
    parser.add_argument("--dataset-name", help="Dataset name to use")
    parser.add_argument("--output", default="run_agent_evaluation.py", help="Output file name")
    args = parser.parse_args()

    print("=" * 60)
    print("MLflow Evaluation Execution Template Generator")
    print("=" * 60)
    print()

    # Check environment
    errors = validate_env_vars()
    if errors:
        print("✗ Environment validation failed:")
        for error in errors:
            print(f"  - {error}")
        print("\nRun scripts/setup_mlflow.py first")
        sys.exit(1)

    tracking_uri = os.getenv("MLFLOW_TRACKING_URI")
    experiment_id = os.getenv("MLFLOW_EXPERIMENT_ID")

    print(f"Tracking URI: {tracking_uri}")
    print(f"Experiment ID: {experiment_id}")
    print()

    # Get agent module (must be specified manually)
    print("Agent module configuration...")
    agent_module = args.module
    if not agent_module:
        print("  ✗ Agent module not specified")
        print("  Use --module to specify your agent module")
        print("  Example: --module my_agent.agent")
        print("\n  To find your agent module:")
        print("    grep -r 'def.*agent' . --include='*.py'")
        sys.exit(1)
    else:
        print(f"  ✓ Using specified: {agent_module}")

    # Get entry point (must be specified manually)
    print("\nEntry point configuration...")
    entry_point = args.entry_point
    if not entry_point:
        print("  ✗ Entry point not specified")
        print("  Use --entry-point to specify your agent's main function")
        print("  Example: --entry-point run_agent")
        print("\n  To find entry points with @mlflow.trace:")
        print("    grep -r '@mlflow.trace' . --include='*.py'")
        sys.exit(1)
    else:
        print(f"  ✓ Using specified: {entry_point}")

    # Get dataset name
    print("\nFetching available datasets...")
    dataset_name = args.dataset_name
    if not dataset_name:
        datasets = list_datasets()

        if datasets:
            print(f"\n✓ Found {len(datasets)} dataset(s):")
            for i, name in enumerate(datasets, 1):
                print(f"  {i}. {name}")

            # Auto-select first dataset
            dataset_name = datasets[0]
            print(f"\n✓ Auto-selected: {dataset_name}")
            print("  (Use --dataset-name to specify a different dataset)")
        else:
            print("  ✗ No datasets found")
            print("  Please create a dataset first or specify with --dataset-name")
            sys.exit(1)
    else:
        print(f"  ✓ Using specified: {dataset_name}")

    # Generate code
    print("\n" + "=" * 60)
    print("Generating Evaluation Execution Script")
    print("=" * 60)

    code = generate_evaluation_code(
        tracking_uri, experiment_id, dataset_name, agent_module, entry_point
    )

    # Write to file
    output_file = args.output
    with open(output_file, "w") as f:
        f.write(code)

    print(f"\n✓ Script generated: {output_file}")
    print()

    # Make executable
    try:
        os.chmod(output_file, 0o755)
        print(f"✓ Made executable: chmod +x {output_file}")
    except Exception:
        pass

    print()
    print("=" * 60)
    print("Next Steps")
    print("=" * 60)
    print()
    print(f"1. Review the generated script: {output_file}")
    print("2. Ensure your entry point function signature matches dataset input keys")
    print(f"3. Execute: uv run python {output_file}")
    print()
    print("The script will:")
    print("  - Load the dataset and registered scorers")
    print("  - Run mlflow.genai.evaluate() with your agent")
    print("  - Save results to evaluation_results.csv")
    print()
    print("=" * 60)


if __name__ == "__main__":
    main()
